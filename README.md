# üìà Stock_It_Up

A modular, deterministic, explainable CLI-based financial decision assistant for equity evaluation using historical stock data (NSE & BSE).

---

# 1Ô∏è‚É£ Problem Understanding

The objective was to build a conversational system that:

* Allows users to explore stock data from NSE and BSE
* Computes performance metrics over a chosen time horizon
* Applies weighted scoring based on user preference
* Adapts to different risk profiles
* Produces ranked recommendations and portfolio allocations
* Clearly explains *why* a stock is recommended
* Simulates portfolio behaviour via Monte Carlo analysis

Key constraints:

* Must not rely entirely on AI models
* Must use real dataset (2000‚Äì2023 historical CSVs)
* Must be deterministic and explainable
* Should follow modular, maintainable architecture

The system acts as a **Decision Companion**, not just a ranking calculator.

---

# 2Ô∏è‚É£ Assumptions Made

1. Historical daily data (2000‚Äì2023) is sufficient to compute CAGR, Volatility, Sharpe, Sortino, MDD, and Avg Volume.
2. Users compare stocks relative to each other, not against the entire market.
3. Scoring is **relative** (min‚Äìmax normalisation across selected stocks).
4. Risk profile modifies weight emphasis rather than altering raw metrics.
5. Risk-free rate is fixed at **6% p.a.** (`config.py`).
6. Stage 1 portfolio volatility (independent-stock approximation) is an **upper bound** on true Sharpe when stocks are positively correlated.
7. Directory structure:

   ```
   comp_stock_data/
       stock_data_NSE/<TICKER>/<TICKER>_<YEAR>.csv
       stock_data_BSE/<TICKER>/<TICKER>_<YEAR>.csv
       .cache/                  ‚Üê auto-built metric cache
   ```

---

# 3Ô∏è‚É£ Architecture

```
ConversationManager      ‚Üí FSM orchestration
IntentParser             ‚Üí NLP / command routing
SessionContext           ‚Üí per-session user state
DataLoader               ‚Üí filesystem I/O + lru_cache
MetricCache              ‚Üí persistent JSON metric cache (fingerprinted)
MetricsEngine            ‚Üí financial calculations (CAGR, Vol, Sharpe, MDD, Sortino)
ScoringEngine            ‚Üí weighted min-max normalisation
ScreenerEngine           ‚Üí market-wide screener (heapq, cache fast-path)
PortfolioEngine          ‚Üí allocation + risk decomposition + covariance + Monte Carlo
ExplanationEngine        ‚Üí deterministic per-stock interpretation
AllocationExplanationEngine ‚Üí portfolio-level explanation + Monte Carlo section
ResponseGenerator        ‚Üí formatting only
```

Each layer has a single responsibility. No cross-layer logic leakage.

---

# 4Ô∏è‚É£ Metrics Implemented

| Metric | Engine | Formula |
|---|---|---|
| **CAGR** | MetricsEngine | `(P_end / P_start)^(1/years) ‚àí 1` |
| **Volatility** | MetricsEngine | Annualised std of log returns |
| **Avg Volume** | MetricsEngine | Mean daily traded volume |
| **Latest Price** | MetricsEngine | Most recent closing price |
| **Sharpe Ratio** | MetricsEngine | `(CAGR ‚àí Rf) / Volatility` |
| **Max Drawdown** | MetricsEngine | `max((Peak ‚àí Trough) / Peak)` |
| **Sortino Ratio** | MetricsEngine | `(CAGR ‚àí Rf) / Downside deviation` |
| **Portfolio CAGR** | PortfolioEngine | `Œ£ w·µ¢ ¬∑ CAGR·µ¢` |
| **Portfolio Vol (Stage 1)** | PortfolioEngine | `‚àö(Œ£ w·µ¢¬≤ ¬∑ œÉ·µ¢¬≤)` ‚Äî independent approx |
| **Portfolio Vol (Stage 2)** | PortfolioEngine | `‚àö(w·µÄ Œ£ w)` ‚Äî covariance-aware |
| **Portfolio Sharpe** | PortfolioEngine | `(Rp ‚àí Rf) / œÉp` |
| **VaR 95%** | PortfolioEngine | 5th percentile of simulated return distribution |
| **CVaR 95%** | PortfolioEngine | Mean of returns below VaR |

---

# 5Ô∏è‚É£ Portfolio Engine ‚Äî 4 Stages

### Stage 1 ‚Äî Independent Approximation
```
œÉp¬≤ = Œ£ w·µ¢¬≤ ¬∑ œÉ·µ¢¬≤
```
Fast. Assumes zero correlation. Documents upper-bound Sharpe assumption.

### Stage 2 ‚Äî Covariance-Aware
```
œÉp¬≤ = w·µÄ Œ£ w
```
Full correlation matrix. Requires user-supplied `covariance_matrix`.

### Stage 3 ‚Äî Factor Models
_Planned ‚Äî CAPM / multi-factor._

### Stage 4 ‚Äî Monte Carlo Simulation ‚úÖ
```
r ~ N(Œº, Œ£)  via Cholesky decomposition
```
Pure-Python Cholesky (`LL·µÄ = Œ£`). 10,000 simulations by default. Returns:
- Mean return, Std dev
- VaR 95%, CVaR 95%
- Probability of loss
- Reproducible via `seed=`

---

# 6Ô∏è‚É£ Screener Mode

Scan the entire exchange by any metric and return top-N stocks.

### Commands
```
top 10 NSE                     ‚Üê top 10 by CAGR (default)
top 10 NSE by cagr             ‚Üê explicit
top 10 NSE by risk             ‚Üê volatility (lowest risk last)
top 10 NSE by volume
top 10 NSE by sharpe
top 10 NSE by sortino
top 10 NSE by drawdown
top 10 NSE by score            ‚Üê weighted multi-metric composite
lowest 10 NSE by volatility    ‚Üê least volatile (safest)
top 5 BSE by risk-adjusted
```

### Full Metric Alias Table

| You type | Metric |
|---|---|
| `by cagr` / `by growth` / `by return` | CAGR |
| `by risk` / `by risky` / `by volatility` / `by volatile` / `by safe` / `by safest` | Volatility |
| `by volume` / `by avg volume` | Avg Volume |
| `by price` / `by latest price` | Latest Price |
| `by sharpe` / `by risk-adjusted` / `by risk adjusted` | Sharpe |
| `by sortino` / `by downside` | Sortino |
| `by drawdown` / `by mdd` / `by max drawdown` | Max Drawdown |
| `by score` / `by rating` / `by ranked` | Composite Score |

### Metric Cache (auto-built)

The first screener query per exchange builds a persistent JSON cache at `comp_stock_data/.cache/<EXCHANGE>_<N>y.json`. All subsequent queries are served from the cache in **< 1 second** instead of ~4 minutes.

- Cache is fingerprinted by SHA-256 of all CSV file mtimes
- Auto-invalidated when CSVs change on disk
- Written atomically (temp ‚Üí rename) ‚Äî crash-safe
- Refresh manually: `rebuild cache NSE` / `refresh cache BSE`

---

# 7Ô∏è‚É£ Allocation Methods

| Method | Logic |
|---|---|
| `proportional` | `w·µ¢ = score_i / Œ£ score_j` |
| `softmax` | `w·µ¢ = e^(score_i) / Œ£ e^(score_j)` |
| `risk_adjusted` | `w·µ¢ ‚àù score_i / volatility_i` |

Constraints: `max_cap` and `min_floor` supported. Last element absorbs rounding remainder to guarantee `Œ£ w·µ¢ = 1` exactly.

---

# 8Ô∏è‚É£ Mathematical Guarantees (Tested)

| Invariant | Tolerance |
|---|---|
| `Œ£ allocation = 1.0` | < 1e-9 |
| `Œ£ risk_share = 1.0` | < 1e-9 |
| `Œ£ capital ‚âà budget` | < 1e-2 (rounding) |
| Stage 2 Vol ‚â§ Stage 1 Vol (positive corr.) | Proven analytically |
| CVaR ‚â§ VaR | Monte Carlo invariant |
| Cholesky `L@L·µÄ = Œ£` | < 1e-10 |

---

# 9Ô∏è‚É£ Discoverability Commands

Work in **any** conversation state:

```
list                     ‚Üí list all tickers (paginated, 50/page)
list NSE                 ‚Üí NSE tickers
list NSE page 3
search TCS               ‚Üí prefix + substring search
help / keywords          ‚Üí show all commands
exchanges / markets      ‚Üí show available exchanges
rebuild cache NSE        ‚Üí force-rebuild metric cache
refresh cache BSE
```

---

# üîü Edge Cases Handled

‚úî Allocation sum = 1.0 (float drift guarded)  
‚úî Risk shares sum = 1.0  
‚úî Capital = budget (rounding absorbed in last element)  
‚úî Zero-score division guarded  
‚úî Non-positive-definite covariance matrix raises ValueError  
‚úî Single-stock portfolio  
‚úî All stocks identical metrics (normalisation division-by-zero guard)  
‚úî Searching partial ticker names  
‚úî Pagination beyond total pages  
‚úî No exchange selected before screener  
‚úî Missing/corrupt CSV silently skipped  
‚úî Cache corruption (invalid JSON) treated as cache miss ‚Üí rebuild  
‚úî Test isolation ‚Äî MetricCache mocked in all ConversationManager tests  

---

# 1Ô∏è‚É£1Ô∏è‚É£ How to Run

### Clone & setup
```bash
git clone <repo-url>
cd Stock_It_Up/Stock_It_Up
python -m venv venv
venv\Scripts\activate        # Windows
pip install -r requirements.txt
```

### Run the CLI
```bash
python main.py
```

### Run Tests
```bash
pytest                       # 389 tests, ~3 seconds
pytest tests/test_metric_cache.py -v   # cache tests only
```

---

# 1Ô∏è‚É£2Ô∏è‚É£ Example Session

```
> top 10 NSE
  [builds cache first time ~4 min, instant thereafter]

> top 10 NSE by sharpe
  [instant from cache]

> top 10 BSE by risk
  [lowest volatility stocks]

> NSE
> 500000
> medium
> 3 years
> TCS INFY WIPRO
> explain TCS
> rebuild cache NSE
```

---

# 1Ô∏è‚É£3Ô∏è‚É£ Known Limitations

| Limitation | Notes |
|---|---|
| Stage 1 Vol ignores correlation | Upper bound. Use Stage 2 with explicit covariance matrix for precision. |
| No transaction costs | Assumed frictionless rebalancing |
| No survivorship bias correction | All tickers in dataset included |
| Risk-free rate static at 6% | Set in `config.py` |
| Covariance matrix must be user-supplied | Stage 2 / Monte Carlo |
| No intraday or options data | Daily OHLC only |
| No sector / market-cap filtering | Future work |

---

# 1Ô∏è‚É£4Ô∏è‚É£ What Remains (Future Work)

- [ ] Stage 3 ‚Äî Factor Models (CAPM / Fama-French)
- [ ] Sector & market-cap awareness (`top IT stocks`)
- [ ] Dynamic risk-free rate (RBI repo rate API)
- [ ] Web / Streamlit interface
- [ ] Benchmark comparison (vs Nifty 50 / Sensex)
- [ ] Percentile-based thresholds for explanation engine

---

# 1Ô∏è‚É£5Ô∏è‚É£ Final Summary

Stock_It_Up is:

* **Deterministic** ‚Äî no randomness except opt-in Monte Carlo seed
* **Explainable** ‚Äî every recommendation comes with a 5-section breakdown
* **Modular** ‚Äî 11 single-responsibility engines
* **Production-grade tested** ‚Äî 389 tests covering mathematical invariants, parser stress, behavioral profiles, and cache lifecycle
* **Fast** ‚Äî screener queries served in < 1 second from fingerprinted cache
* **Risk-aware** ‚Äî 4 stages of portfolio risk modeling from simple to full simulation

It transforms historical stock data into a structured decision-support system rather than a simple ranking script.

---
